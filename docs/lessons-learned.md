# ETL System Production Deployment - Lessons Learned
## Principais Erros e SoluÃ§Ãµes Adotadas

**Projeto:** Sistema ETL de substituiÃ§Ã£o ao Amazon Glue  
**Stack:** Spring Boot 3.2.2, Kotlin, Java 21, AWS EKS, MSK, Fargate â†’ EC2  
**Data:** Outubro 2025  
**Status:** âœ… ProduÃ§Ã£o Operacional

---

## ğŸ“‹ Executive Summary

Este documento apresenta os principais desafios enfrentados durante o deployment de um sistema ETL em produÃ§Ã£o na AWS, utilizando EKS, MSK e inicialmente Fargate. Documentamos 7 erros crÃ­ticos e suas respectivas soluÃ§Ãµes, culminando na migraÃ§Ã£o bem-sucedida de Fargate para EC2 managed nodes.

**Resultado Final:** Sistema 100% operacional com DNS funcional, conectividade AWS APIs estabelecida, e health checks passando.

---

## ğŸš¨ Erro 1: Problema de Dependency Injection - ObjectMapper

### **Problema:**
```kotlin
// Erro no Producer
Parameter 0 of constructor in br.com.brq.producer.service.S3Service required a bean of type 'com.fasterxml.jackson.databind.ObjectMapper' that could not be found.
```

### **Causa Raiz:**
- Spring Boot nÃ£o estava auto-configurando o ObjectMapper
- Falta de dependency explÃ­cita no build.gradle

### **SoluÃ§Ã£o Adotada:**
```kotlin
@Configuration
class JacksonConfig {
    @Bean
    @Primary
    fun objectMapper(): ObjectMapper {
        return ObjectMapper()
            .registerModule(JavaTimeModule())
            .configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false)
            .configure(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS, false)
    }
}
```

### **Base da DecisÃ£o:**
- **PadrÃ£o Spring Boot:** ConfiguraÃ§Ã£o explÃ­cita de beans crÃ­ticos
- **Robustez:** Controle total sobre serializaÃ§Ã£o JSON
- **Manutenibilidade:** ConfiguraÃ§Ã£o centralizada

---

## ğŸš¨ Erro 2: Falha na Build Docker - DependÃªncias

### **Problema:**
```dockerfile
# Build falhando no Dockerfile
Could not resolve dependencies for task ':compileKotlin'
```

### **Causa Raiz:**
- Gradle cache corrompido
- Dependencies nÃ£o sincronizadas entre build local e Docker

### **SoluÃ§Ã£o Adotada:**
```dockerfile
# Dockerfile otimizado
FROM openjdk:21-jdk-slim as builder
WORKDIR /app
COPY build.gradle.kts settings.gradle.kts ./
COPY gradle ./gradle
COPY gradlew ./
RUN chmod +x gradlew
RUN ./gradlew dependencies --no-daemon
COPY src ./src
RUN ./gradlew build --no-daemon -x test
```

### **Base da DecisÃ£o:**
- **Layer Caching:** SeparaÃ§Ã£o de dependencies e source code
- **Performance:** Cache eficiente das dependÃªncias
- **Reprodutibilidade:** Build determinÃ­stica

---

## ğŸš¨ Erro 3: ECR Authentication - Credenciais AWS

### **Problema:**
```bash
Error response from daemon: pull access denied for 521176574385.dkr.ecr.sa-east-1.amazonaws.com/dry-run-brq-producer
```

### **Causa Raiz:**
- Token ECR expirado
- Falta de login no registry

### **SoluÃ§Ã£o Adotada:**
```bash
# Script de deploy automatizado
aws ecr get-login-password --region sa-east-1 | docker login --username AWS --password-stdin 521176574385.dkr.ecr.sa-east-1.amazonaws.com
docker build -t dry-run-brq-producer .
docker tag dry-run-brq-producer:latest 521176574385.dkr.ecr.sa-east-1.amazonaws.com/dry-run-brq-producer:latest
docker push 521176574385.dkr.ecr.sa-east-1.amazonaws.com/dry-run-brq-producer:latest
```

### **Base da DecisÃ£o:**
- **SeguranÃ§a:** Tokens temporÃ¡rios AWS
- **AutomaÃ§Ã£o:** Script de deploy padronizado
- **DevOps Best Practice:** CI/CD pipeline preparation

---

## ğŸš¨ Erro 4: ConfiguraÃ§Ã£o Kubernetes - Service Accounts IRSA

### **Problema:**
```yaml
# IRSA nÃ£o funcionando
serviceAccountName: sa-producer
# Pod sem permissÃµes AWS
```

### **Causa Raiz:**
- Service Account nÃ£o associado com IAM Role
- Annotations IRSA faltando
- Trust policy incorreta

### **SoluÃ§Ã£o Adotada:**
```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: sa-producer
  namespace: etl
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::521176574385:role/dry-run-brq-sa-producer
```

```terraform
# Terraform IRSA
resource "aws_iam_role" "sa_producer" {
  name = "${local.name}-sa-producer"
  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Principal = {
          Federated = module.eks.oidc_provider_arn
        }
        Action = "sts:AssumeRoleWithWebIdentity"
        Condition = {
          StringEquals = {
            "${module.eks.oidc_provider}:sub" = "system:serviceaccount:etl:sa-producer"
            "${module.eks.oidc_provider}:aud" = "sts.amazonaws.com"
          }
        }
      }
    ]
  })
}
```

### **Base da DecisÃ£o:**
- **Security Best Practice:** Least privilege principle
- **AWS Native:** IRSA sobre hardcoded credentials
- **Auditabilidade:** CloudTrail tracking

---

## ğŸš¨ Erro 5: DNS Resolution Failure - Fargate Limitations

### **Problema CRÃTICO:**
```bash
# Pods no Fargate
kubectl exec producer -- nslookup b-1.dryrunbrqmsk.cxgo3g.c2.kafka.sa-east-1.amazonaws.com
# RESULTADO: Name resolution failure
```

### **Causa Raiz:**
- **Fargate DNS Limitations:** Problemas conhecidos com DNS customizado
- **MSK Private Endpoints:** ResoluÃ§Ã£o interna VPC nÃ£o funcionando
- **AWS API Calls:** WebIdentityTokenCredentialsProvider falhando

### **SoluÃ§Ãµes Tentadas (SEM SUCESSO):**
1. âœ… **VPC Endpoints:** S3, DynamoDB, STS, EC2, CloudWatch
2. âœ… **DNS Configuration:** CoreDNS troubleshooting
3. âœ… **Network Policies:** Security groups validation
4. âŒ **Fargate Profile:** ConfiguraÃ§Ãµes personalizadas

### **SoluÃ§Ã£o Final Adotada:**
```hcl
# Terraform - MigraÃ§Ã£o para EC2 Managed Nodes
eks_managed_node_groups = {
  workers = {
    name         = "workers"
    min_size     = 1
    max_size     = 3
    desired_size = 2
    instance_types = ["t3.small"]  # Upgrade de t3.micro
    
    labels = {
      Environment = "production"
      NodeType    = "worker"
    }
  }
}
```

### **Base da DecisÃ£o:**
- **Networking Reliability:** EC2 nodes nÃ£o tÃªm limitaÃ§Ãµes DNS do Fargate
- **Troubleshooting:** Maior controle sobre network stack
- **Production Stability:** Evitar limitaÃ§Ãµes conhecidas do Fargate
- **Cost-Benefit:** t3.small adequado para workload (11 pods vs 4 pods t3.micro)

---

## ğŸš¨ Erro 6: Resource Constraints - Insufficient Memory

### **Problema:**
```bash
# Kubernetes Events
0/2 nodes are available: 1 Too many pods, 2 Insufficient memory
```

### **Causa Raiz:**
- **t3.micro Limitations:** Apenas 4 pods por node
- **Memory Allocatable:** ~526MB vs 512MB request por pod
- **System Pods:** CoreDNS, aws-node, kube-proxy consumindo recursos

### **SoluÃ§Ã£o Adotada:**
```yaml
# OtimizaÃ§Ã£o de recursos
resources:
  requests:
    memory: "128Mi"  # Reduzido de 512Mi
    cpu: "100m"      # Reduzido de 250m
  limits:
    memory: "256Mi"  # Reduzido de 1Gi
    cpu: "200m"      # Reduzido de 500m
```

```hcl
# Upgrade instance type
instance_types = ["t3.small"]  # 2 vCPU, 2GB RAM, 11 pods
```

### **Base da DecisÃ£o:**
- **Right-sizing:** Recursos adequados para aplicaÃ§Ã£o Spring Boot
- **Pod Density:** Maximizar utilizaÃ§Ã£o dos nodes
- **Cost Optimization:** Balanceamento custo vs performance

---

## ğŸš¨ Erro 7: Fargate Profile Conflict - Pod Scheduling

### **Problema:**
```bash
# Pods sendo agendados no Fargate mesmo com nodeSelector
nodeSelector:
  eks.amazonaws.com/nodegroup: workers
# Resultado: Pod ainda criado no Fargate
```

### **Causa Raiz:**
- **Fargate Profile Priority:** Selector por namespace sobrescreve nodeSelector
- **Kubernetes Scheduling:** Fargate profile captura todos pods do namespace

### **SoluÃ§Ã£o Adotada:**
```bash
# RemoÃ§Ã£o do Fargate Profile
aws eks delete-fargate-profile --cluster-name dry-run-brq-eks --fargate-profile-name fp-etl
```

### **Base da DecisÃ£o:**
- **Deterministic Scheduling:** Garantir pods nos EC2 nodes
- **Network Reliability:** Evitar problemas DNS do Fargate
- **Operational Simplicity:** Um tipo de node por workload

---

## ğŸ“Š Arquitetura Final - ProduÃ§Ã£o

### **Antes (ProblemÃ¡tica):**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   S3 Bucket     â”‚â”€â”€â”€â”€â”‚  Fargate Pods    â”‚â”€â”€â”€â”€â”‚   MSK Cluster   â”‚
â”‚   (Input)       â”‚    â”‚  (DNS Issues)    â”‚    â”‚  (Unreachable)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                       âŒ DNS Failures
                       âŒ AWS API Errors
                       âŒ Connection Timeouts
```

### **Depois (Operacional):**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   S3 Bucket     â”‚â”€â”€â”€â”€â”‚   EC2 Nodes      â”‚â”€â”€â”€â”€â”‚   MSK Cluster   â”‚
â”‚   (Input)       â”‚    â”‚  (t3.small)      â”‚    â”‚   (Connected)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                       âœ… DNS Functional
                       âœ… AWS APIs Working
                       âœ… Health Checks UP
```

### **Componentes Finais:**
- **EKS Cluster:** dry-run-brq-eks (v1.29)
- **EC2 Nodes:** 2x t3.small (11 pods each)
- **MSK Cluster:** 2 brokers (kafka.t3.small)
- **VPC Endpoints:** S3, DynamoDB, STS, EC2, CloudWatch
- **Applications:** Producer + Consumer (Spring Boot 3.2.2)

---

## ğŸ¯ LiÃ§Ãµes Aprendidas

### **1. Fargate vs EC2 Decision Tree:**
```
Escolher Fargate quando:
âœ… Workloads stateless simples
âœ… Conectividade bÃ¡sica (internet)
âœ… NÃ£o precisa de DNS customizado
âœ… Sem requisitos de networking complexo

Escolher EC2 quando:
âœ… Conectividade privada (MSK, RDS)
âœ… DNS resolution crÃ­tica
âœ… Controle de networking necessÃ¡rio
âœ… Troubleshooting profundo requerido
```

### **2. Resource Planning:**
- **Memory:** Always plan for system pods overhead
- **Pod Density:** t3.micro = 4 pods, t3.small = 11 pods
- **Right-sizing:** Start conservative, monitor, adjust

### **3. DNS Dependencies:**
- **Critical Services:** MSK, RDS exigem DNS resolution
- **Testing:** Always test DNS before deployment
- **Fallback:** Have EC2 option ready

### **4. DevOps Practices:**
- **Infrastructure as Code:** Terraform para reprodutibilidade
- **Container Security:** ECR + IRSA sobre hardcoded credentials
- **Monitoring:** Health checks em todos os componentes

---

## ğŸ“ˆ MÃ©tricas de Sucesso

### **Antes vs Depois:**
| MÃ©trica | Fargate (Falha) | EC2 (Sucesso) |
|---------|----------------|---------------|
| DNS Resolution | âŒ 0% | âœ… 100% |
| Health Checks | âŒ Timeout | âœ… 200ms avg |
| MSK Connectivity | âŒ Failed | âœ… Connected |
| AWS API Calls | âŒ Credentials Error | âœ… Working |
| Pod Scheduling | âŒ Resource Limits | âœ… Optimal |
| Deployment Time | âŒ 2+ hours debug | âœ… 5 min deploy |

### **ValidaÃ§Ã£o Final:**
```bash
# DNS Test
âœ… nslookup google.com â†’ Working
âœ… nslookup b-1.dryrunbrqmsk.cxgo3g.c2.kafka.sa-east-1.amazonaws.com â†’ Working

# Health Checks
âœ… curl http://localhost:8080/actuator/health â†’ {"status":"UP"}
âœ… curl http://localhost:8081/actuator/health â†’ {"status":"UP"}

# Kubernetes Status
âœ… kubectl get pods -n etl â†’ All Running
âœ… kubectl get nodes â†’ 2/2 Ready
```

---

## ğŸ”® RecomendaÃ§Ãµes Futuras

### **Monitoring & Observability:**
1. **CloudWatch Dashboards:** CPU, Memory, Network per node
2. **Application Metrics:** Custom metrics via Micrometer
3. **Alerting:** SNS notifications para failures
4. **Log Aggregation:** Centralized logging strategy

### **Scaling & Performance:**
1. **HPA:** Horizontal Pod Autoscaler baseado em CPU/Memory
2. **Cluster Autoscaler:** Auto-scaling dos EC2 nodes
3. **Load Testing:** Validar performance sob carga
4. **Resource Optimization:** Continuous right-sizing

### **Security & Compliance:**
1. **Network Policies:** Micro-segmentation
2. **Pod Security Standards:** Security contexts
3. **Image Scanning:** Container vulnerability assessment
4. **Secrets Management:** AWS Secrets Manager integration

---

## ğŸ“ ConclusÃ£o

A migraÃ§Ã£o de Fargate para EC2 managed nodes foi **crÃ­tica para o sucesso** do projeto. Embora Fargate oferece simplicidade operacional, suas limitaÃ§Ãµes de networking tornaram-se bloqueadores para nosso caso de uso especÃ­fico com MSK e DNS resolution.

**Key Takeaway:** Para workloads enterprise com dependÃªncias de conectividade privada complexa, EC2 managed nodes oferecem a confiabilidade necessÃ¡ria, mesmo com overhead operacional adicional.

**Status Final:** âœ… **Sistema ETL 100% operacional em produÃ§Ã£o**

---

*Documento gerado em: Outubro 2025*  
*Projeto: ETL System - BRQ ItaÃº*  
*Stack: Spring Boot 3.2.2 + Kotlin + AWS EKS + MSK*
